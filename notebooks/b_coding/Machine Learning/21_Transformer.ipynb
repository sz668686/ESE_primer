{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Transformer)=\n",
    "# Chapter 21 -- Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers are a type of deep learning model introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017. They have revolutionized natural language processing (NLP) by enabling the creation of models like BERT, GPT, and T5. Transformers leverage self-attention mechanisms to process input data in parallel, making them highly efficient and effective for various tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mathematical Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self Attention Mechanism\n",
    "The self-attention mechanism is a core component of the transformer model. It allows the model to weigh the importance of different words in a sequence when encoding a particular word.\n",
    "\n",
    "#### Query, Key, and Value Vectors\n",
    "For each word in the input sequence, we create three vectors: Query ($Q$), Key ($K$), and Value ($V$). These vectors are obtained by multiplying the input embeddings with learned weight matrices.\n",
    "\n",
    "$$\n",
    "Q = XW_Q \\quad K = XW_K \\quad V = XW_V\n",
    "$$\n",
    "(eq2_1)\n",
    "\n",
    "Where:\n",
    "- $X$ is the input embedding matrix.\n",
    "- $ W_Q, W_K, W_V $ are learned weight matrices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Score Calculation \n",
    "The attention scores are calculated using the dot product of the query and key vectors, followed by scaling.\n",
    "\n",
    "$$\n",
    "\\text{Attention\\_scores} = \\frac{QK^T}{\\sqrt{d_k}}\n",
    "$$\n",
    "(eq2_2)\n",
    "\n",
    "Where:\n",
    "- $d_k$ is the dimension of the key vectors (used for scaling).\n",
    "\n",
    "#### Softmax Function for Normalization\n",
    "\n",
    "To convert the attention scores into probabilities, we apply the softmax function.\n",
    "\n",
    "$$\n",
    "\\alpha_{ij} = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) = \\frac{\\exp \\left(\\frac{Q_i K_j^T}{\\sqrt{d_k}}\\right)}{\\sum_{j=1}^{n} \\exp \\left(\\frac{Q_i K_j^T}{\\sqrt{d_k}}\\right)}\n",
    "$$\n",
    "(eq2_3)\n",
    "\n",
    "#### Weighted Sum of Values\n",
    "\n",
    "The output of the self-attention mechanism for each word is the weighted sum of the value vectors, where the weights are the attention scores.\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\alpha V\n",
    "$$\n",
    "(eq2_4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Positional Encoding\n",
    "\n",
    "Since transformers do not have a built-in notion of word order, we add positional encodings to the input embeddings to incorporate sequence order information.\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\n",
    "$$\n",
    "(eq2_5)\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\n",
    "$$\n",
    "(eq2_6)\n",
    "\n",
    "Where:\n",
    "- $pos$ is the position.\n",
    "- $i$ is the dimension.\n",
    "- $d_{model}$ is the model dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Scaled Dot-Product Attention\n",
    "\n",
    "Scaled dot-product attention uses the concepts described above (queries, keys, values, and softmax normalization).\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "(eq2_7)\n",
    "\n",
    "### 2.4 Multi-Head Attention\n",
    "\n",
    "Instead of performing a single attention function, multi-head attention runs multiple attention mechanisms in parallel, which allows the model to focus on different parts of the input sequence.\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, \\ldots, \\text{head}_h)W_O\n",
    "$$\n",
    "(eq2_8)\n",
    "\n",
    "Where each head is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(QW_{Q_i}, KW_{K_i}, VW_{V_i})\n",
    "$$\n",
    "(eq2_9)\n",
    "\n",
    "### 2.5 Feed-Forward Neural Networks\n",
    "\n",
    "Each position in the sequence is processed independently and identically by a fully connected feed-forward network (FFN), which consists of two linear transformations with a ReLU activation in between.\n",
    "\n",
    "$$\n",
    "\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n",
    "$$\n",
    "(eq2_10)\n",
    "\n",
    "### 2.6 Layer Normalization and Residual Connections\n",
    "\n",
    "Transformers use layer normalization and residual connections to stabilize and speed up the training process.\n",
    "\n",
    "#### Layer Normalization\n",
    "\n",
    "$$\n",
    "\\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sigma + \\epsilon} \\gamma + \\beta\n",
    "$$\n",
    "(eq2_11)\n",
    "\n",
    "Where:\n",
    "- $mu$ is the mean.\n",
    "- $sigma$ is the standard deviation.\n",
    "- $gamma$ and $beta$ are learned parameters.\n",
    "\n",
    "#### Residual Connection\n",
    "\n",
    "$$\n",
    "\\text{Output} = \\text{LayerNorm}(x + \\text{Sublayer}(x))\n",
    "$$\n",
    "(eq2_12)\n",
    "\n",
    "Where Sublayer could be the multi-head attention mechanism or the feed-forward network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3. Transformer Architecture Diagram\n",
    "\n",
    "A detailed diagram of the transformer architecture shows the encoder and decoder blocks. Key components include multi-head attention and feed-forward layers.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Structure of Transformer\n",
    "\n",
    "### 4.1 Encoder\n",
    "\n",
    "The encoder is composed of a stack of $N$ identical layers. Each layer has two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network.\n",
    "\n",
    "#### Encoder Layer\n",
    "\n",
    "Each encoder layer can be described mathematically as:\n",
    "\n",
    "$$\n",
    "\\text{LayerNorm}(x + \\text{MultiHead}(x, x, x)) \\quad \\text{(Self-Attention Sub-Layer)}\n",
    "$$\n",
    "(eq4_1)\n",
    "\n",
    "$$\n",
    "\\text{LayerNorm}(x + \\text{FFN}(x)) \\quad \\text{(Feed-Forward Sub-Layer)}\n",
    "$$\n",
    "(eq4_2)\n",
    "\n",
    "Where:\n",
    "- $x$ is the input to the encoder layer.\n",
    "- $\\text{MultiHead}$ is the multi-head attention mechanism.\n",
    "- $\\text{FFN}$ is the feed-forward neural network.\n",
    "\n",
    "### 4.2 Decoder\n",
    "\n",
    "The decoder is also composed of a stack of $N$ identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.\n",
    "\n",
    "#### Decoder Layer\n",
    "\n",
    "Each decoder layer can be described mathematically as:\n",
    "\n",
    "$$\n",
    "\\text{LayerNorm}(x + \\text{MultiHead}(x, x, x)) \\quad \\text{(Masked Self-Attention Sub-Layer)}\n",
    "$$\n",
    "(eq4_3)\n",
    "\n",
    "$$\n",
    "\\text{LayerNorm}(x + \\text{MultiHead}(x, \\text{EncoderOutput}, \\text{EncoderOutput})) \\quad \\text{(Encoder-Decoder Attention Sub-Layer)}\n",
    "$$\n",
    "(eq4_4)\n",
    "\n",
    "$$\n",
    "\\text{LayerNorm}(x + \\text{FFN}(x)) \\quad \\text{(Feed-Forward Sub-Layer)}\n",
    "$$\n",
    "(eq4_5)\n",
    "\n",
    "Where:\n",
    "- $x$ is the input to the decoder layer.\n",
    "- $\\text{EncoderOutput}$ is the output from the encoder stack.\n",
    "\n",
    "### 4.3 Encoder-Decoder Interactions\n",
    "\n",
    "The encoder and decoder interact through the encoder-decoder attention sub-layer. This layer allows the decoder to focus on relevant parts of the input sequence. The mathematical representation of this interaction is given by:\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, \\ldots, \\text{head}_h)W_O\n",
    "$$\n",
    "(eq2_8)\n",
    "\n",
    "Where each attention head is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(QW_{Q_i}, KW_{K_i}, VW_{V_i})\n",
    "$$\n",
    "(eq2_9)\n",
    "\n",
    "And the attention function is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "(eq2_7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implmentation a Transformer from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout)\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        src = self.embedding(src) * math.sqrt(d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        tgt = self.embedding(tgt) * math.sqrt(d_model)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        output = self.transformer(src, tgt, src_mask, tgt_mask)\n",
    "        return output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Example usage:\n",
    "vocab_size = 10000  # Define vocabulary size\n",
    "d_model = 512       # Embedding dimension\n",
    "nhead = 8           # Number of attention heads\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 6\n",
    "dim_feedforward = 2048\n",
    "dropout = 0.1\n",
    "\n",
    "model = Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout)\n",
    "src = torch.randint(0, vocab_size, (10, 32))  \n",
    "tgt = torch.randint(0, vocab_size, (20, 32))  \n",
    "output = model(src, tgt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
