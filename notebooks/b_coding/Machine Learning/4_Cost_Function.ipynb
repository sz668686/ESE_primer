{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "module-dsml"
    ]
   },
   "source": [
    "(Cost_Function)= \n",
    "# Chapter 4 -- Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous 2 examples have the same amount of students, so it is a fair comparison. However, if they have different sizes, then the very accurate model with 1000 data would not have better cross-entropy result than the one with 5 data. So we need to divide the amount of the data to make `per capita' comparison. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Cost = -\\frac{1}{m}\\sum_{i=1}^m [y_i*ln(p_i)+(1-y_i)*ln(1-p_i)]\n",
    "$$ (eq4_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the $p_i$ is the AI's prediction of the probability of the event happening, which is the same as $\\hat{y}$ notation we used before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{y} = \\sigma(\\vec{w}*\\vec{x}+b)\n",
    "$$ (eq4_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So finally, we can obtain the following by replacing $p_i$ by $\\hat{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Cost(\\vec{w},b) = -\\frac{1}{m}\\sum_{i=1}^m [y_i*ln( \\sigma(\\vec{w}*\\vec{x}+b))+(1-y_i)*ln(1- \\sigma(\\vec{w}*\\vec{x}+b))]\n",
    "$$ (eq4_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want is to find the weights $w$ and bias $b$ to minimise the cost. This is a simple multi-variable calculus problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Theory\n",
    "\n",
    "### Sigmoid Function\n",
    "\n",
    "The sigmoid function, $\\sigma(z)$, is often used in logistic regression and neural networks to map any real-valued number into the range of 0 to 1. It is defined as:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "To minimize the cost function, we use an optimization algorithm called gradient descent. Gradient descent updates the weights $\\vec{w}$ and bias $b$ iteratively to find the minimum of the cost function. The update rules are given by:\n",
    "\n",
    "$$\n",
    "\\vec{w} := \\vec{w} - \\alpha \\frac{\\partial}{\\partial \\vec{w}} Cost(\\vec{w}, b)\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\frac{\\partial}{\\partial b} Cost(\\vec{w}, b)\n",
    "$$\n",
    "\n",
    "\n",
    "where $\\alpha$ is the learning rate.\n",
    "\n",
    "### Derivatives of the Cost Function\n",
    "\n",
    "The partial derivatives of the cost function with respect to the weights and bias are:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\vec{w}} Cost(\\vec{w}, b) = \\frac{1}{m} \\sum_{i=1}^m (\\hat{y}_i - y_i) \\vec{x}_i\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial b} Cost(\\vec{w}, b) = \\frac{1}{m} \\sum_{i=1}^m (\\hat{y}_i - y_i)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Cost 0.6883\n",
      "Iteration 100: Cost 0.6192\n",
      "Iteration 200: Cost 0.5918\n",
      "Iteration 300: Cost 0.5665\n",
      "Iteration 400: Cost 0.5431\n",
      "Iteration 500: Cost 0.5216\n",
      "Iteration 600: Cost 0.5017\n",
      "Iteration 700: Cost 0.4832\n",
      "Iteration 800: Cost 0.4661\n",
      "Iteration 900: Cost 0.4503\n",
      "Final weights: [ 0.91260242 -0.20690417]\n",
      "Final bias: -1.1195065937669517\n",
      "Final cost: 0.4357\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Cost function\n",
    "def compute_cost(X, y, w, b):\n",
    "    m = X.shape[0]\n",
    "    cost = -(1/m) * np.sum(y * np.log(sigmoid(X.dot(w) + b)) + (1 - y) * np.log(1 - sigmoid(X.dot(w) + b)))\n",
    "    return cost\n",
    "\n",
    "# Gradient descent\n",
    "def gradient_descent(X, y, w, b, alpha, num_iterations):\n",
    "    m = X.shape[0]\n",
    "    cost_history = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # Compute the predictions\n",
    "        predictions = sigmoid(X.dot(w) + b)\n",
    "        \n",
    "        # Compute the gradients\n",
    "        dw = (1/m) * X.T.dot(predictions - y)\n",
    "        db = (1/m) * np.sum(predictions - y)\n",
    "        \n",
    "        # Update the weights and bias\n",
    "        w -= alpha * dw\n",
    "        b -= alpha * db\n",
    "        \n",
    "        # Compute and store the cost\n",
    "        cost = compute_cost(X, y, w, b)\n",
    "        cost_history.append(cost)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}: Cost {cost:.4f}\")\n",
    "    \n",
    "    return w, b, cost_history\n",
    "\n",
    "# Example data\n",
    "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n",
    "y = np.array([0, 0, 1, 1])\n",
    "\n",
    "# Initialize parameters\n",
    "w = np.zeros(X.shape[1])\n",
    "b = 0\n",
    "alpha = 0.01\n",
    "num_iterations = 1000\n",
    "\n",
    "# Perform gradient descent\n",
    "w, b, cost_history = gradient_descent(X, y, w, b, alpha, num_iterations)\n",
    "\n",
    "# Final weights and bias\n",
    "print(f\"Final weights: {w}\")\n",
    "print(f\"Final bias: {b}\")\n",
    "\n",
    "# Final cost\n",
    "final_cost = compute_cost(X, y, w, b)\n",
    "print(f\"Final cost: {final_cost:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
