{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Hyper_parameters)=\n",
    "# Chapter 19 -- Hyper-Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Vision Transformer (ViT) is a breakthrough architecture that applies the principles of the Transformer model, originally designed for natural language processing (NLP), to the domain of computer vision. Introduced by Dosovitskiy et al. in their 2020 paper titled \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,\" the ViT has revolutionized the way deep learning models handle image data, offering a powerful alternative to traditional convolutional neural networks (CNNs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying Transformers on images was always going to be a challenge for the following reasons,\n",
    "\n",
    "Unlike words/sentences/paragraphs, images contain much much more information in them basically in form of pixels.\n",
    "\n",
    "It would be very hard, even with current hardware to attend to every other pixel in the image.\n",
    "\n",
    "Instead, a popular alternative was to use localized attention.\n",
    "\n",
    "In fact CNNs do something very similar through convolutions and the receptive field essentially grows bigger as we go deeper into the model's layers, but Tranformers were always going to be computationally more expensive than CNNs because of the' nature of Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer (ViT): A Comprehensive Review\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The Vision Transformer (ViT) is a groundbreaking model that applies the Transformer architecture, initially developed for natural language processing (NLP), to image-based tasks. ViT was introduced by Dosovitskiy et al. in their 2020 paper, \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.\" The model has shown that Transformers can outperform traditional convolutional neural networks (CNNs) in various computer vision tasks, particularly when trained on large datasets.\n",
    "\n",
    "## Key Concepts and Theoretical Foundations\n",
    "\n",
    "### 1. Image as a Sequence of Patches\n",
    "\n",
    "Unlike CNNs, which process images as grids of pixels with localized filters, the Vision Transformer treats an image as a sequence of patches. The idea is to break down the image into smaller, fixed-size patches, which can be processed similarly to words in NLP tasks.\n",
    "\n",
    "Given an input image \\( \\mathbf{x} \\) of size \\( H \\times W \\times C \\) (height, width, and number of channels), the image is divided into patches of size \\( P \\times P \\). The total number of patches \\( N \\) is given by:\n",
    "\n",
    "$$\n",
    "N = \\frac{H \\times W}{P^2}\n",
    "$$\n",
    "\n",
    "Each patch \\( \\mathbf{x}_p \\) is then flattened into a vector of size \\( P^2 \\times C \\). This vector is linearly projected into a higher-dimensional space \\( \\mathbb{R}^D \\) using a learnable matrix \\( \\mathbf{E} \\):\n",
    "\n",
    "$$\n",
    "\\mathbf{z}_p = \\mathbf{E} \\cdot \\text{flatten}(\\mathbf{x}_p)\n",
    "$$\n",
    "\n",
    "where \\( \\mathbf{z}_p \\in \\mathbb{R}^D \\) is the embedded patch.\n",
    "\n",
    "### 2. Positional Embedding\n",
    "\n",
    "Transformers are inherently permutation-invariant, meaning they do not have a built-in mechanism to capture the order of input tokens. To retain the spatial structure of the image, a learnable positional embedding \\( \\mathbf{E}_{pos} \\) is added to each patch embedding:\n",
    "\n",
    "$$\n",
    "\\mathbf{z}_p' = \\mathbf{z}_p + \\mathbf{E}_{pos}\n",
    "$$\n",
    "\n",
    "This positional embedding provides the model with information about the relative positions of patches within the image, enabling it to maintain spatial relationships.\n",
    "\n",
    "### 3. Transformer Encoder\n",
    "\n",
    "The sequence of embedded patches, now augmented with positional information, is fed into a Transformer encoder. The encoder is composed of multiple layers, each containing a multi-head self-attention mechanism and a feed-forward network (FFN).\n",
    "\n",
    "#### Multi-Head Self-Attention\n",
    "\n",
    "The self-attention mechanism allows the model to compute the relationships between different patches in the sequence, capturing both local and global dependencies. For a given input sequence of patches \\( \\mathbf{Z} = [\\mathbf{z}_1', \\mathbf{z}_2', \\dots, \\mathbf{z}_N'] \\), the attention mechanism computes the output as follows:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q} \\mathbf{K}^\\top}{\\sqrt{D_k}}\\right) \\mathbf{V}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( \\mathbf{Q} = \\mathbf{Z} \\mathbf{W}_Q \\), \\( \\mathbf{K} = \\mathbf{Z} \\mathbf{W}_K \\), and \\( \\mathbf{V} = \\mathbf{Z} \\mathbf{W}_V \\) are the query, key, and value matrices, respectively.\n",
    "- \\( \\mathbf{W}_Q, \\mathbf{W}_K, \\mathbf{W}_V \\) are learnable weight matrices.\n",
    "- \\( D_k \\) is the dimensionality of the key/query vectors.\n",
    "\n",
    "In the multi-head attention mechanism, this process is repeated \\( h \\) times with different sets of projection matrices, and the results are concatenated and linearly transformed:\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h) \\mathbf{W}_O\n",
    "$$\n",
    "\n",
    "Where ( \\mathbf{W}_O \\) is another learnable weight matrix.\n",
    "\n",
    "#### Position-Wise Feed-Forward Network\n",
    "\n",
    "After the self-attention mechanism, the output is passed through a position-wise feed-forward network (FFN), which is applied independently to each position in the sequence:\n",
    "\n",
    "$$\n",
    "\\text{FFN}(\\mathbf{x}) = \\text{ReLU}(\\mathbf{x} \\mathbf{W}_1 + \\mathbf{b}_1) \\mathbf{W}_2 + \\mathbf{b}_2\n",
    "$$\n",
    "\n",
    "Where \\( \\mathbf{W}_1 \\), \\( \\mathbf{W}_2 \\), \\( \\mathbf{b}_1 \\), and \\( \\mathbf{b}_2 \\) are learnable parameters.\n",
    "\n",
    "### 4. Classification Token (CLS Token)\n",
    "\n",
    "To generate a global representation of the image, ViT introduces a special classification token (`[CLS]`) at the beginning of the sequence of patch embeddings. This token is treated like any other patch and is processed through the Transformer encoder. After passing through the encoder, the output corresponding to the `[CLS]` token is used as the global representation of the image:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\text{softmax}(\\mathbf{W}_{\\text{cls}} \\mathbf{z}_{\\text{CLS}})\n",
    "$$\n",
    "\n",
    "Where \\( \\mathbf{W}_{\\text{cls}} \\) is a learnable matrix, and \\( \\mathbf{z}_{\\text{CLS}} \\) is the output corresponding to the `[CLS]` token.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ViT, an image is treated as a sequence of patches, similar to how words are treated in NLP tasks. The image is divided into fixed-size patches (e.g., 16x16 or 32x32 pixels), and each patch is flattened into a vector. This approach simplifies the training process, as smaller patches reduce the complexity of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Vision Transformer, images are treated as sequences rather than grids of pixels. The image is divided into fixed-size patches (e.g., 16x16 or 32x32 pixels), and each patch is considered analogous to a \"word\" in NLP. This division simplifies the training process, as smaller patches reduce the model's complexity and improve its ability to learn. The phrase \"An Image is Worth 16x16 Words\" captures this idea, highlighting that each image can be broken down into a sequence of patches for further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "\n",
    "from self_attention_cv import TransformerEncoder\n",
    "\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *,\n",
    "                 img_dim,\n",
    "                 in_channels=3,\n",
    "                 patch_dim=16,\n",
    "                 num_classes=10,\n",
    "                 dim=512,\n",
    "                 blocks=6,\n",
    "                 heads=4,\n",
    "                 dim_linear_block=1024,\n",
    "                 dim_head=None,\n",
    "                 dropout=0, transformer=None, classification=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_dim: the spatial image size\n",
    "            in_channels: number of img channels\n",
    "            patch_dim: desired patch dim\n",
    "            num_classes: classification task classes\n",
    "            dim: the linear layer's dim to project the patches for MHSA\n",
    "            blocks: number of transformer blocks\n",
    "            heads: number of heads\n",
    "            dim_linear_block: inner dim of the transformer linear block\n",
    "            dim_head: dim head in case you want to define it. defaults to dim/heads\n",
    "            dropout: for pos emb and transformer\n",
    "            transformer: in case you want to provide another transformer implementation\n",
    "            classification: creates an extra CLS token\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert img_dim % patch_dim == 0, f'patch size {patch_dim} not divisible'\n",
    "        self.p = patch_dim\n",
    "        self.classification = classification\n",
    "        tokens = (img_dim // patch_dim) ** 2\n",
    "        self.token_dim = in_channels * (patch_dim ** 2)\n",
    "        self.dim = dim\n",
    "        self.dim_head = (int(dim / heads)) if dim_head is None else dim_head\n",
    "        self.project_patches = nn.Linear(self.token_dim, dim)\n",
    "\n",
    "        self.emb_dropout = nn.Dropout(dropout)\n",
    "        if self.classification:\n",
    "            self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "            self.pos_emb1D = nn.Parameter(torch.randn(tokens + 1, dim))\n",
    "            self.mlp_head = nn.Linear(dim, num_classes)\n",
    "        else:\n",
    "            self.pos_emb1D = nn.Parameter(torch.randn(tokens, dim))\n",
    "\n",
    "        if transformer is None:\n",
    "            self.transformer = TransformerEncoder(dim, blocks=blocks, heads=heads,\n",
    "                                                  dim_head=self.dim_head,\n",
    "                                                  dim_linear_block=dim_linear_block,\n",
    "                                                  dropout=dropout)\n",
    "        else:\n",
    "            self.transformer = transformer\n",
    "\n",
    "    def expand_cls_to_batch(self, batch):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch: batch size\n",
    "        Returns: cls token expanded to the batch size\n",
    "        \"\"\"\n",
    "        return self.cls_token.expand([batch, -1, -1])\n",
    "\n",
    "    def forward(self, img, mask=None):\n",
    "        batch_size = img.shape[0]\n",
    "        img_patches = rearrange(\n",
    "            img, 'b c (patch_x x) (patch_y y) -> b (x y) (patch_x patch_y c)',\n",
    "                                patch_x=self.p, patch_y=self.p)\n",
    "        # project patches with linear layer + add pos emb\n",
    "        img_patches = self.project_patches(img_patches)\n",
    "\n",
    "        if self.classification:\n",
    "            img_patches = torch.cat(\n",
    "                (self.expand_cls_to_batch(batch_size), img_patches), dim=1)\n",
    "\n",
    "        patch_embeddings = self.emb_dropout(img_patches + self.pos_emb1D)\n",
    "\n",
    "        # feed patch_embeddings and output of transformer. shape: [batch, tokens, dim]\n",
    "        y = self.transformer(patch_embeddings, mask)\n",
    "\n",
    "        if self.classification:\n",
    "            # we index only the cls token for classification. nlp tricks :P\n",
    "            return self.mlp_head(y[:, 0, :])\n",
    "        else:\n",
    "            return y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
